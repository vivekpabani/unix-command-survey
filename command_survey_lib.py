import json
import re
import numpy as np
import pandas as pd
from collections import namedtuple

countline = re.compile("^\s*(\d+) (\S+)\s*")
Person = namedtuple('Person', 'work email data')

def get_people(dataframe):
    """ 
    This function parses user_responses collected from survey.

    :param dataframe
        (pd.DataFrame)  : a dataframe with following columns:
                          1. "What are your top 100 unix commands?"
                          2. "What kind of work do you do? (optional)"
                          3. "What is your email address? (optional)"

                          Values in the first column are multiple lines seperated by new line char,
                          where each line is "<freq_of_command> <command_name>"
                          Each row corresponds to one user's response.

    :return (Generator) : Person with fields work, email and command_data.  
                                    
    """
    for i, row in dataframe.iterrows():
        topcommands = row["What are your top 100 unix commands?"]

        if pd.isnull(topcommands):
            continue
        lines = topcommands.split("\n")
        matching_lines = [line for line in lines if countline.match(line) is not None]

        if len(matching_lines) == 0:
            continue

        countdata = pd.DataFrame([countline.match(line).groups()
                                    for line in matching_lines],
                                    columns = ['count', 'command'])
        countdata['count'] = np.vectorize(np.int)(countdata['count'])

        if len(set(countdata['command'])) != len(countdata['command']):
            # Add up all the counts for the same command
            countdata = countdata.groupby('command').aggregate('sum').reset_index()

        countdata['normalized_count'] = countdata['count'] / float(sum(countdata['count']))
        countdata['num_people_using'] = 1
        countdata = countdata[countdata['command'] != 'historynsed']

        yield Person(row["What kind of work do you do? (optional)"],
                    row["What is your email address? (optional)"],
                    countdata)


def command_summary(people):
    """
    This function analyses processed user responses (Person.data field)
    and returns a dataframe with commands sorted by number of people using them.

    :param people (list) : list of Person generated by get_people function.

    :return pd.DataFrame : a dataframe with following columns, sorted by num_people_using:
                            1. command
                            2. count
                            3. num_people_using
                            4. usage_frequency

                            where count is total number of times command was found
                            in all user's history.
            
    """

    everyone = pd.concat([p.data for p in people])
    g = everyone.groupby('command')
    cmds = g.aggregate(sum).sort_values(by=['num_people_using'], ascending=False)
    cmds['usage_frequency'] = cmds['count'] / cmds['num_people_using']

    return cmds


def command_usage_vectors(cmds, people):
    """
    This function uses the command summary and processed responses,
    and generates a dataframe with a binary vector per person to indicate
    which commands were used by the person.

    :param cmds (pd.DataFrame) : dataframe generated by command_summary function

    :param people (list): list of Person generated by get_people function.

    :return pd.DataFrame : a dataframe with 'n' columns and 'm' rows,
                            where n is number of users, and m is number of commands.
                            each values is either 0 or 1 indicating whether that
                            person used that command or not.

    """

    usage_vectors = pd.DataFrame(columns=np.arange(len(people)),
                                index=cmds[cmds.num_people_using > 1].index)
    usage_vectors.fillna(0, inplace=True)

    for i, person in enumerate(people):
        to_set = person.data[person.data['command'].isin(usage_vectors.index)]['command']
        usage_vectors[i].ix[to_set] = 1

    return usage_vectors


def compute_correlations(usage_vectors, num_commands=50):
    """
    This function calculates correlations between each pair of commands
    by comparing their usage among all people.

    :param usage_vectors
            (pd.DataFrame): transpose of usage_vectors generated by command_usage_vectors
                            function, so that each command makes a column, and each person
                            makes a row.
    :param num_commands
                    (int):  number of commands to consider to make pairs and
                            calculate correlation.

    :return pd.DataFrame :  a dataframe with columns:
                            1. one
                            2. two
                            3. correlation
                            where 'one' and 'two' are the command names, and 'correlation'
                            is the calculated correlation between them.
    """

    def get_corr(a, b):
        return usage_vectors[a].corr(usage_vectors[b])

    common_columns = usage_vectors.columns[:num_commands]

    return pd.DataFrame([(i, j, get_corr(i, j)) 
                            for i in common_columns 
                            for j in common_columns 
                            if i < j], 
                        columns = ['one', 'two', 'correlation'])


def get_connected_components(correlations):
    """
    This function makes group of those commands which are correlated to each other.
    It scans through filtered correlations dataframe and checks each pair of commands:
        if one of the commands is present in an already created group, then it adds both
        these commands to that group.
        otherwise it creates a new group and add both commands to the new group.

    :param correlations
            (pd.DataFrame):  dataframe generated by compute_correlations function,
                            but with filter (correlation > 0.25). This filter is applied
                            in correlation_graph function.

    :return list: list of sets, where each set is a group of commands correlated to each other.
    """

    together_things = []

    for _, row in correlations.iterrows():
        command1, command2, correlation = row.values

        for thingset in together_things:

            if command1 in thingset or command2 in thingset:
                thingset.add(command1)
                thingset.add(command2)
                break
        else:
            together_things.append(set([command1, command2]))

    return together_things

def correlation_graph(correlations):
    """
    This function creates the json for graph with link and node details.

    :param correlations
            (pd.DataFrame): dataframe generated by compute_correlations function

    :return dict: dictionary with two keys:
                    "nodes": list of dicts, where each dict has name of command and a group id
                            based on which group it belongs to (calculated by get_connected_components)
                    "links": list of dicts, where each dict has a source and target command,
                            and their correlation value.
    """

    nodes_to_graph = correlations[correlations['correlation'] > 0.25]
    command_list = list(set(list(nodes_to_graph.one.unique()) + list(nodes_to_graph.two.unique())))
    command_indices = {name: idx for idx, name in enumerate(command_list)}
    connected_components = get_connected_components(nodes_to_graph)
    component_indices = {cmd: idx for idx, commands in enumerate(connected_components) for cmd in commands}

    def make_dict(row):

        return {
            "source": command_indices[row['one']], 
            "target": command_indices[row['two']], 
            "value": row['correlation']}

    nodes = [{'name': cmd, "group": component_indices[cmd]} for cmd in command_list]
    links = [ make_dict(row) for _, row in nodes_to_graph.iterrows()]

    return {
        "nodes": nodes,
        "links": links
    }

def main():

    # if you have the actual responses from the survey,
    # then start from here.
    user_responses = pd.read_csv("user_responses.csv");
    people = list(get_people(user_responses))
    commands = command_summary(people)
    usage_vectors = command_usage_vectors(commands, people)
    usage_vectors = usage_vectors.transpose()

    # if you have the usage_vectors.csv and not the user responses,
    # then comment everything till this point, and uncomment following
    # three lines to load usage_vectors from file and sanitize data.

    ##usage_vectors = pd.read_csv("usage_vectors.csv")
    ##usage_vectors = usage_vectors.set_index("command").transpose()
    ##usage_vectors = usage_vectors.loc[:, usage_vectors.columns.notnull()]

    # second argument is number of commands to consider for correlations.
    # use 'len(usage_vectors)' to consider all commands.
    correlations = compute_correlations(usage_vectors, 200)

    graph = correlation_graph(correlations)

    with open("unix-command-graph.json", "w+") as f:
        json.dump(graph, f)

if __name__ == "__main__":

    main()
